{"cells":[{"cell_type":"markdown","source":["# Fine Tune BERT on Patent dataset"],"metadata":{"id":"HFJOLmxZXTSK"}},{"cell_type":"markdown","metadata":{"id":"9wrd7Toh2oRj"},"source":["# Installation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FOMxr-qf2mNd"},"outputs":[],"source":["!pip install transformers==4.2.1"]},{"cell_type":"markdown","metadata":{"id":"JJZKQepzzOLo"},"source":["# Import"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rozmCrwSSLCl"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import transformers"]},{"cell_type":"markdown","metadata":{"id":"OoTIpJEQzLm7"},"source":["# Configuration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UGZ84aCQSLFi"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZQEhzsuvSLHw"},"outputs":[],"source":["PROJECT_DIR = \"/content/drive/MyDrive/patent\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m1aAe9f8SLKf"},"outputs":[],"source":["DATA_DIR = PROJECT_DIR + '/data/'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"08iI9rRpSLNb"},"outputs":[],"source":["TRAIN_FILE = DATA_DIR + 'patent_train.csv'\n","VALIDATION_FILE = DATA_DIR + 'patent_validation.csv'\n","TEST_FILE = DATA_DIR + 'patent_test.csv'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fKxaHkaMSLQy"},"outputs":[],"source":["MODEL_NAME = 'bert-base-uncased'\n","OUTPUT_NAME = 'patent_' + MODEL_NAME + '_sup-bert-keras'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XF6f5hgowe9b"},"outputs":[],"source":["OUTPUT_PATH = DATA_DIR + '/results/' + OUTPUT_NAME"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N76Qxx5fSLS5"},"outputs":[],"source":["CACHE_DIR = DATA_DIR + 'cache'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wYzmpFBESLVX"},"outputs":[],"source":["DF_SEP = \"|\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jafKuZKB1wW2"},"outputs":[],"source":["sent0_cname = \"anchor\"\n","sent1_cname = \"target\"\n","sent2_cname = \"score\""]},{"cell_type":"markdown","metadata":{"id":"WPKIVBH2zXfN"},"source":["# HyperParameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PmAXaphmzXpW"},"outputs":[],"source":["max_length = 128  # Maximum length of input sentence to the model.\n","batch_size = 32\n","epochs = 2\n","labels = [\"0.0\", \"0.25\", \"0.5\", \"0.75\", \"1.0\"]\n","num_classes = len(labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2B5JJlPhzqLe"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"SOq1lBVRzq3O"},"source":["# Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fBEdOxPVSLXw"},"outputs":[],"source":["train_df = pd.read_csv(TRAIN_FILE, sep = DF_SEP)\n","valid_df = pd.read_csv(VALIDATION_FILE, sep = DF_SEP)\n","test_df = pd.read_csv(TEST_FILE, sep = DF_SEP)\n","\n","# Shape of the data\n","print(f\"Total train samples : {train_df.shape[0]}\")\n","print(f\"Total validation samples: {valid_df.shape[0]}\")\n","print(f\"Total test samples: {test_df.shape[0]}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FgTF2tHpz4yI"},"outputs":[],"source":["print(f\"Sentence1: {train_df.loc[1, sent0_cname]}\")\n","print(f\"Sentence2: {train_df.loc[1, sent1_cname]}\")\n","print(f\"Similarity: {train_df.loc[1, sent2_cname]}\")"]},{"cell_type":"markdown","metadata":{"id":"3S-qzgmcz_y4"},"source":["# Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gqpmc10Pz7KL"},"outputs":[],"source":["print(\"Number of missing values\")\n","print(train_df.isnull().sum())\n","train_df.dropna(axis=0, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Xo1WQXRz9at"},"outputs":[],"source":["print(\"Train Target Distribution\")\n","print(train_df[sent2_cname].value_counts())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n7DHloUA0CgN"},"outputs":[],"source":["print(\"Validation Target Distribution\")\n","print(valid_df[sent2_cname].value_counts())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ljQBvWYo0HOd"},"outputs":[],"source":["train_df[\"label\"] = train_df[sent2_cname]\n","y_train = tf.keras.utils.to_categorical(train_df.label, num_classes=num_classes)\n","\n","valid_df[\"label\"] = valid_df[sent2_cname]\n","y_val = tf.keras.utils.to_categorical(valid_df.label, num_classes=num_classes)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"69UfCdMz0M_U"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"XhpcLKTH0L6l"},"source":["# Data Generator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z4VPvcN50Otu"},"outputs":[],"source":["class BertSemanticDataGenerator(tf.keras.utils.Sequence):\n","    \"\"\"Generates batches of data.\n","\n","    Args:\n","        sentence_pairs: Array of premise and hypothesis input sentences.\n","        labels: Array of labels.\n","        batch_size: Integer batch size.\n","        shuffle: boolean, whether to shuffle the data.\n","        include_targets: boolean, whether to incude the labels.\n","\n","    Returns:\n","        Tuples `([input_ids, attention_mask, `token_type_ids], labels)`\n","        (or just `[input_ids, attention_mask, `token_type_ids]`\n","         if `include_targets=False`)\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        sentence_pairs,\n","        labels,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        include_targets=True,\n","    ):\n","        self.sentence_pairs = sentence_pairs\n","        self.labels = labels\n","        self.shuffle = shuffle\n","        self.batch_size = batch_size\n","        self.include_targets = include_targets\n","        # Load our BERT Tokenizer to encode the text.\n","        # We will use base-base-uncased pretrained model.\n","        self.tokenizer = transformers.BertTokenizer.from_pretrained(MODEL_NAME, do_lower_case=True\n","        )\n","        self.indexes = np.arange(len(self.sentence_pairs))\n","        self.on_epoch_end()\n","\n","    def __len__(self):\n","        # Denotes the number of batches per epoch.\n","        return len(self.sentence_pairs) // self.batch_size\n","\n","    def __getitem__(self, idx):\n","        # Retrieves the batch of index.\n","        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n","        sentence_pairs = self.sentence_pairs[indexes]\n","\n","        # With BERT tokenizer's batch_encode_plus batch of both the sentences are\n","        # encoded together and separated by [SEP] token.\n","        encoded = self.tokenizer.batch_encode_plus(\n","            sentence_pairs.tolist(),\n","            add_special_tokens=True,\n","            max_length=max_length,\n","            return_attention_mask=True,\n","            return_token_type_ids=True,\n","            pad_to_max_length=True,\n","            return_tensors=\"tf\",\n","        )\n","\n","        # Convert batch of encoded features to numpy array.\n","        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n","        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n","        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n","\n","        # Set to true if data generator is used for training/validation.\n","        if self.include_targets:\n","            labels = np.array(self.labels[indexes], dtype=\"int32\")\n","            return [input_ids, attention_masks, token_type_ids], labels\n","        else:\n","            return [input_ids, attention_masks, token_type_ids]\n","\n","    def on_epoch_end(self):\n","        # Shuffle indexes after each epoch if shuffle is set to True.\n","        if self.shuffle:\n","            np.random.RandomState(42).shuffle(self.indexes)"]},{"cell_type":"markdown","metadata":{"id":"VHq3ho_k0Sy0"},"source":["# Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J4VFLOPg0S-1"},"outputs":[],"source":["# Create the model under a distribution strategy scope.\n","strategy = tf.distribute.MirroredStrategy()\n","\n","with strategy.scope():\n","    # Encoded token ids from BERT tokenizer.\n","    input_ids = tf.keras.layers.Input(\n","        shape=(max_length,), dtype=tf.int32, name=\"input_ids\"\n","    )\n","    # Attention masks indicates to the model which tokens should be attended to.\n","    attention_masks = tf.keras.layers.Input(\n","        shape=(max_length,), dtype=tf.int32, name=\"attention_masks\"\n","    )\n","    # Token type ids are binary masks identifying different sequences in the model.\n","    token_type_ids = tf.keras.layers.Input(\n","        shape=(max_length,), dtype=tf.int32, name=\"token_type_ids\"\n","    )\n","    # Loading pretrained BERT model.\n","    bert_model = transformers.TFBertModel.from_pretrained(MODEL_NAME)\n","    # Freeze the BERT model to reuse the pretrained features without modifying them.\n","    bert_model.trainable = False\n","\n","    bert_output = bert_model.bert(\n","        input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n","    )\n","    sequence_output = bert_output.last_hidden_state\n","    pooled_output = bert_output.pooler_output\n","    # Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.\n","    bi_lstm = tf.keras.layers.Bidirectional(\n","        tf.keras.layers.LSTM(64, return_sequences=True)\n","    )(sequence_output)\n","    # Applying hybrid pooling approach to bi_lstm sequence output.\n","    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n","    max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n","    concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n","    dropout = tf.keras.layers.Dropout(0.3)(concat)\n","    output = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(dropout)\n","    model = tf.keras.models.Model(\n","        inputs=[input_ids, attention_masks, token_type_ids], outputs=output\n","    )\n","\n","    model.compile(\n","        optimizer=tf.keras.optimizers.Adam(),\n","        loss=\"categorical_crossentropy\",\n","        metrics=[\"acc\"],\n","    )\n","\n","\n","print(f\"Strategy: {strategy}\")\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"Ix38lCU50XqP"},"source":["# Batch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6eu6Cq7t0X8X"},"outputs":[],"source":["train_data = BertSemanticDataGenerator(\n","    train_df[[sent0_cname, sent1_cname]].values.astype(\"str\"),\n","    y_train,\n","    batch_size=batch_size,\n","    shuffle=True,\n",")\n","valid_data = BertSemanticDataGenerator(\n","    valid_df[[sent0_cname, sent1_cname]].values.astype(\"str\"),\n","    y_val,\n","    batch_size=batch_size,\n","    shuffle=False,\n",")"]},{"cell_type":"markdown","metadata":{"id":"BKKHgqw_0cut"},"source":["# Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BII10Z2K0c3L"},"outputs":[],"source":["history = model.fit(\n","    train_data,\n","    validation_data=valid_data,\n","    epochs=epochs,\n","    use_multiprocessing=True,\n","    workers=-1,\n",")"]},{"cell_type":"markdown","metadata":{"id":"LhvjrVb_0fzb"},"source":["# Fine Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZUCRqh840f5p"},"outputs":[],"source":["# Unfreeze the bert_model.\n","bert_model.trainable = True\n","# Recompile the model to make the change effective.\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adam(1e-5),\n","    loss=\"categorical_crossentropy\",\n","    metrics=[\"accuracy\"],\n",")\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"i1L5r2ZQ0lE5"},"source":["# Train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"KBFcW6ZX0lKj"},"outputs":[],"source":["history = model.fit(\n","    train_data,\n","    validation_data=valid_data,\n","    epochs=epochs,\n","    use_multiprocessing=True,\n","    workers=-1,\n",")"]},{"cell_type":"markdown","metadata":{"id":"SmKPQS1QDWXX"},"source":["# Save"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"4fsJiG0WDV7c"},"outputs":[],"source":["model.save(OUTPUT_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"JP7Aq3Qk8xNK"},"outputs":[],"source":["%$%$^&"]},{"cell_type":"markdown","metadata":{"id":"p2ggEOnrC0OI"},"source":["# Predict"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"zVg1oSR3CzvS"},"outputs":[],"source":["test_data = BertSemanticDataGenerator(\n","    test_df[[sent0_cname, sent1_cname]].values.astype(\"str\"),\n","    y_test,\n","    batch_size=batch_size,\n","    shuffle=False,\n",")\n","model.evaluate(test_data, verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"_C1X-BoMCzx3"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"8UQy-DVZCz0b"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"oZBMLnkx0vWW"},"source":["# Test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"h3xq-oyc0vdu"},"outputs":[],"source":["test_data = BertSemanticDataGenerator(\n","    test_df[[sent0_cname, sent1_cname]].values.astype(\"str\"),\n","    y_test,\n","    batch_size=batch_size,\n","    shuffle=False,\n",")\n","model.evaluate(test_data, verbose=1)"]},{"cell_type":"markdown","metadata":{"id":"2AEein9h0yuW"},"source":["# Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"mKTpr70n0yzw"},"outputs":[],"source":["def check_similarity(sentence1, sentence2):\n","    sentence_pairs = np.array([[str(sentence1), str(sentence2)]])\n","    test_data = BertSemanticDataGenerator(\n","        sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,\n","    )\n","\n","    proba = model.predict(test_data[0])[0]\n","    idx = np.argmax(proba)\n","    proba = f\"{proba[idx]: .2f}%\"\n","    pred = labels[idx]\n","    return pred, proba"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"SL6FbzehSLae"},"outputs":[],"source":["sentence1 = \"Two women are observing something together.\"\n","sentence2 = \"Two women are standing with their eyes closed.\"\n","check_similarity(sentence1, sentence2)"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"collapsed_sections":["SOq1lBVRzq3O","3S-qzgmcz_y4","XhpcLKTH0L6l","VHq3ho_k0Sy0","Ix38lCU50XqP","BKKHgqw_0cut","LhvjrVb_0fzb","i1L5r2ZQ0lE5","SmKPQS1QDWXX","p2ggEOnrC0OI","oZBMLnkx0vWW","2AEein9h0yuW"],"authorship_tag":"ABX9TyMhgP7Sxfi8KoPuc9Y6hvNq"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}