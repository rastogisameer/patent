{"cells":[{"cell_type":"markdown","source":["# Train PromCSE on Patent Dataset"],"metadata":{"id":"aG_CcIA4XaIZ"}},{"cell_type":"markdown","metadata":{"id":"DnVUuKYbu3kG"},"source":["# Installation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"56osVAvOorGs"},"outputs":[],"source":["!pip install datasets==1.2.1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_PA9dyYWo_iL"},"outputs":[],"source":["!pip install transformers==4.2.1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0O-Km-7T2gbh"},"outputs":[],"source":["!pip install dill==0.3.4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jGKem_0upCtt"},"outputs":[],"source":["!pip install tqdm"]},{"cell_type":"markdown","metadata":{"id":"sOUkenftu9CG"},"source":[]},{"cell_type":"markdown","metadata":{"id":"ikBWQsmMzhzC"},"source":["# Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"goeQwFbUqTYL"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B2yfpY07qw_5"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-r_nM8ku8HBR"},"outputs":[],"source":["!ls /content/drive/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NEs90mmbjXQ6"},"outputs":[],"source":["PROJECT_DIR = \"/content/drive/MyDrive/patent\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZmS_41Zj64ET"},"outputs":[],"source":["import sys\n","import os\n","py_file_location = PROJECT_DIR + \"/py_files\"\n","sys.path.append(os.path.abspath(py_file_location))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CiHPFFyss98x"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"BKb4LPrHlFDG"},"source":["# Import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oozaikeRFZUL"},"outputs":[],"source":["import logging\n","import math\n","import os\n","import sys\n","from dataclasses import dataclass, field\n","from typing import Optional, Union, List, Dict, Tuple\n","import torch\n","import collections\n","import random\n","\n","from datasets import load_dataset\n","\n","import transformers\n","from transformers import (\n","    CONFIG_MAPPING,\n","    MODEL_FOR_MASKED_LM_MAPPING,\n","    AutoConfig,\n","    AutoModelForMaskedLM,\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","    DataCollatorForLanguageModeling,\n","    DataCollatorWithPadding,\n","    HfArgumentParser,\n","    Trainer,\n","    TrainingArguments,\n","    default_data_collator,\n","    set_seed,\n","    EvalPrediction,\n","    BertModel,\n","    BertForPreTraining,\n","    RobertaModel\n",")\n","from transformers.tokenization_utils_base import BatchEncoding, PaddingStrategy, PreTrainedTokenizerBase\n","from transformers.trainer_utils import is_main_process\n","from transformers.data.data_collator import DataCollatorForLanguageModeling\n","from transformers.file_utils import cached_property, torch_required, is_torch_available, is_torch_tpu_available\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qljr-OEPk_sW"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y8dsPPhDkTyM"},"outputs":[],"source":["from promcse.models import RobertaForCL, BertForCL\n","from promcse.trainers import CLTrainer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"15O5NnQ9Fnly"},"outputs":[],"source":["import senteval"]},{"cell_type":"markdown","metadata":{"id":"kAWxk8JdlPem"},"source":["# Configuration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xxBCNaBjlYnN"},"outputs":[],"source":["DATA_DIR = PROJECT_DIR + '/data/'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7K3kpFr6tDNZ"},"outputs":[],"source":["#TRAIN_FILE = DATA_DIR + 'patent_train_simcse2.csv' \n","#TRAIN_FILE = DATA_DIR + 'nli_for_simcse.csv'\n","TRAIN_FILE = DATA_DIR + 'patent_snli.csv'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NiQHqAhOQQ44"},"outputs":[],"source":["do_train = True\n","do_eval = False\n","do_test = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ItC46HiHtQjo"},"outputs":[],"source":["logger = logging.getLogger(__name__)\n","MODEL_CONFIG_CLASSES = list(MODEL_FOR_MASKED_LM_MAPPING.keys())\n","MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I2F53eQjtQmB"},"outputs":[],"source":["MODEL_NAME = 'bert-base-uncased'\n","#MODEL_NAME = 'roberta-large'\n","OUTPUT_NAME = 'patent_' + MODEL_NAME + '_sup-dcpcse'\n","#OUTPUT_NAME = 'nli_' + MODEL_NAME + '_sup-dcpcse'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IQKn7ngEFSd8"},"outputs":[],"source":["CACHE_DIR = DATA_DIR + 'cache'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JV-GlxOW4kV9"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"-tmYl4YrBDxB"},"source":["# Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yECR3zgGtDzH"},"outputs":[],"source":["@dataclass\n","class ModelArguments:\n","    \"\"\"\n","    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n","    \"\"\"\n","\n","    # Huggingface's original arguments\n","    model_name_or_path: Optional[str] = field(\n","        default=MODEL_NAME,\n","        metadata={\n","            \"help\": \"The model checkpoint for weights initialization.\"\n","            \"Don't set if you want to train a model from scratch.\"\n","        },\n","    ) \n","    model_type: Optional[str] = field(\n","        default=None,\n","        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n","    )\n","    config_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n","    )\n","    tokenizer_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n","    )\n","    cache_dir: Optional[str] = field(\n","        default=CACHE_DIR,\n","        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n","    )\n","    use_fast_tokenizer: bool = field(\n","        default=True,\n","        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n","    )\n","    model_revision: str = field(\n","        default=\"main\",\n","        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n","    )\n","    use_auth_token: bool = field(\n","        default=False,\n","        metadata={\n","            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n","            \"with private models).\"\n","        },\n","    )\n","\n","    # SimCSE's arguments\n","    temp: float = field(\n","        default=0.05,\n","        metadata={\n","            \"help\": \"Temperature for softmax.\"\n","        }\n","    )\n","    pooler_type: str = field(\n","        default=\"cls\",\n","        metadata={\n","            \"help\": \"What kind of pooler to use (cls, cls_before_pooler, avg, avg_top2, avg_first_last).\"\n","        }\n","    ) \n","    hard_negative_weight: float = field(\n","        default=0,\n","        metadata={\n","            \"help\": \"The **logit** of weight for hard negatives (only effective if hard negatives are used).\"\n","        }\n","    )\n","    do_mlm: bool = field(\n","        default=False,\n","        metadata={\n","            \"help\": \"Whether to use MLM auxiliary objective.\"\n","        }\n","    )\n","    mlm_weight: float = field(\n","        default=0.1,\n","        metadata={\n","            \"help\": \"Weight for MLM auxiliary objective (only effective if --do_mlm).\"\n","        }\n","    )\n","    mlp_only_train: bool = field(\n","        default=False,\n","        metadata={\n","            \"help\": \"Use MLP only during training\"\n","        }\n","    )\n","    pre_seq_len: int = field(\n","        default=12,\n","        metadata={\n","            \"help\": \"The length of prompt\"\n","        }\n","    )\n","    prefix_projection: bool = field(\n","        default=False,\n","        metadata={\n","            \"help\": \"Apply a two-layer MLP head over the prefix embeddings\"\n","        }\n","    ) \n","    prefix_hidden_size: int = field(\n","        default=512,\n","        metadata={\n","            \"help\": \"The hidden size of the MLP projection head in Prefix Encoder if prefix projection is used\"\n","        }\n","    )\n","    do_eh_loss: bool = field(\n","        default=False,\n","        metadata={\n","            \"help\": \"Whether to add Energy-based Hinge loss\"\n","        }\n","    )\n","    eh_loss_margin: float = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"The margin of Energy-based Hinge loss\"\n","        }\n","    )\n","    eh_loss_weight: float = field(\n","        default=10.0,\n","        metadata={\n","            \"help\": \"The weight of Energy-based Hinge loss\"\n","        }\n","    ) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ULPho57ymY4H"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rsZJXS-jtD1i"},"outputs":[],"source":["@dataclass\n","class DataTrainingArguments:\n","    \"\"\"\n","    Arguments pertaining to what data we are going to input our model for training and eval.\n","    \"\"\"\n","\n","    # Huggingface's original arguments. \n","    dataset_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n","    )\n","    dataset_config_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n","    )\n","    overwrite_cache: bool = field(\n","        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n","    )\n","    validation_split_percentage: Optional[int] = field(\n","        default=5,\n","        metadata={\n","            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n","        },\n","    )\n","    preprocessing_num_workers: Optional[int] = field(\n","        default=None,\n","        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n","    )\n","\n","    # SimCSE's arguments\n","    train_file: Optional[str] = field(\n","        default=TRAIN_FILE, \n","        metadata={\"help\": \"The training data file (.txt or .csv).\"}\n","    )\n","\n","\n","    max_seq_length: Optional[int] = field(\n","        default=32,\n","        metadata={\n","            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n","            \"than this will be truncated.\"\n","        },\n","    )\n","    pad_to_max_length: bool = field(\n","        default=False,\n","        metadata={\n","            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n","            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n","        },\n","    )\n","    mlm_probability: float = field(\n","        default=0.15, \n","        metadata={\"help\": \"Ratio of tokens to mask for MLM (only effective if --do_mlm)\"}\n","    )\n","\n","    def __post_init__(self):\n","        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n","            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n","        else:\n","            if self.train_file is not None:\n","                extension = self.train_file.split(\".\")[-1]\n","                assert extension in [\"csv\", \"json\", \"txt\"], \"`train_file` should be a csv, a json or a txt file.\"\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ONegBqJdmZk-"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QnHhDjYXtD4K"},"outputs":[],"source":["@dataclass\n","class OurTrainingArguments(TrainingArguments):\n","    # Evaluation\n","    ## By default, we evaluate STS (dev) during training (for selecting best checkpoints) and evaluate \n","    ## both STS and transfer tasks (dev) at the end of training. Using --eval_transfer will allow evaluating\n","    ## both STS and transfer tasks (dev) during training.\n","    eval_transfer: bool = field(\n","        default=False, \n","        metadata={\"help\": \"Evaluate transfer task dev sets (in validation).\"}\n","    )\n","\n","    output_dir: Optional[str] = field(\n","        default=DATA_DIR + 'results/' + OUTPUT_NAME, \n","        metadata={\"help\": \"The training data file (.txt or .csv).\"}\n","    )\n","    \n","    ##########################################################################\n","    weight_decay: float = field(\n","        default=0.0,\n","        metadata={\"help\": \"Evaluate transfer task dev sets (in validation).\"}\n","    )\n","    ##########################################################################\n","\n","    @cached_property\n","    @torch_required\n","    def _setup_devices(self) -> \"torch.device\":\n","        logger.info(\"PyTorch: setting up devices\")\n","        if self.no_cuda:\n","            device = torch.device(\"cpu\")\n","            self._n_gpu = 0\n","        elif is_torch_tpu_available():\n","            device = xm.xla_device()\n","            self._n_gpu = 0\n","        elif self.local_rank == -1:\n","            # if n_gpu is > 1 we'll use nn.DataParallel.\n","            # If you only want to use a specific subset of GPUs use `CUDA_VISIBLE_DEVICES=0`\n","            # Explicitly set CUDA to the first (index 0) CUDA device, otherwise `set_device` will\n","            # trigger an error that a device index is missing. Index 0 takes into account the\n","            # GPUs available in the environment, so `CUDA_VISIBLE_DEVICES=1,2` with `cuda:0`\n","            # will use the first GPU in that env, i.e. GPU#1\n","            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","            # Sometimes the line in the postinit has not been run before we end up here, so just checking we're not at\n","            # the default value.\n","            self._n_gpu = torch.cuda.device_count()\n","        else:\n","            # Here, we'll use torch.distributed.\n","            # Initializes the distributed backend which will take care of synchronizing nodes/GPUs\n","            #\n","            # deepspeed performs its own DDP internally, and requires the program to be started with:\n","            # deepspeed  ./program.py\n","            # rather than:\n","            # python -m torch.distributed.launch --nproc_per_node=2 ./program.py\n","            if self.deepspeed:\n","                from .integrations import is_deepspeed_available\n","\n","                if not is_deepspeed_available():\n","                    raise ImportError(\"--deepspeed requires deepspeed: `pip install deepspeed`.\")\n","                import deepspeed\n","\n","                deepspeed.init_distributed()\n","            else:\n","                torch.distributed.init_process_group(backend=\"nccl\")\n","            device = torch.device(\"cuda\", self.local_rank)\n","            self._n_gpu = 1\n","\n","        if device.type == \"cuda\":\n","            torch.cuda.set_device(device)\n","\n","        return device\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uumRzTXpmaU6"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VAwz4OHNtD6V"},"outputs":[],"source":["parser = HfArgumentParser((ModelArguments, DataTrainingArguments, OurTrainingArguments))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fyp-GhjrtD89"},"outputs":[],"source":["model_args, data_args, training_args = parser.parse_args_into_dataclasses(args=[])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cWU9qKOPtD_n"},"outputs":[],"source":["set_seed(training_args.seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RdqY1G7ntECE"},"outputs":[],"source":["data_files = {}\n","\n","data_files[\"train\"] = data_args.train_file\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zle972tjYcqh"},"outputs":[],"source":["extension = data_args.train_file.split(\".\")[-1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MckkFHtYtEE5"},"outputs":[],"source":["datasets = load_dataset(extension, data_files=data_files, cache_dir=model_args.cache_dir, delimiter=\",\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k5VMjqnwyrFR"},"outputs":[],"source":["config_kwargs = {\n","    \"cache_dir\": model_args.cache_dir,\n","    \"revision\": model_args.model_revision,\n","    \"use_auth_token\": True if model_args.use_auth_token else None,\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wycSH6-oyrHt"},"outputs":[],"source":["config = AutoConfig.from_pretrained(model_args.model_name_or_path,\n","                                    **config_kwargs) #"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ilONAtpUyrKh"},"outputs":[],"source":["tokenizer_kwargs = {\n","    \"cache_dir\": model_args.cache_dir,\n","    \"use_fast\": model_args.use_fast_tokenizer,\n","    \"revision\": model_args.model_revision,\n","    \"use_auth_token\": True if model_args.use_auth_token else None,\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9_x3lkyayrND"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, **tokenizer_kwargs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n2yRdXBUoin8"},"outputs":[],"source":["if model_args.model_name_or_path:\n","    if 'roberta' in model_args.model_name_or_path:\n","            model = RobertaForCL.from_pretrained(\n","                model_args.model_name_or_path,\n","                from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n","                config=config,\n","                cache_dir=model_args.cache_dir,\n","                revision=model_args.model_revision,\n","                use_auth_token=True if model_args.use_auth_token else None,\n","                model_args=model_args\n","        )\n","    elif 'bert' in model_args.model_name_or_path:\n","        model = BertForCL.from_pretrained(\n","            model_args.model_name_or_path,\n","            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n","            config=config,\n","            cache_dir=model_args.cache_dir,\n","            revision=model_args.model_revision,\n","            use_auth_token=True if model_args.use_auth_token else None,\n","            model_args=model_args\n","        )\n","        if model_args.do_mlm:\n","            pretrained_model = BertForPreTraining.from_pretrained(model_args.model_name_or_path)\n","            model.lm_head.load_state_dict(pretrained_model.cls.predictions.state_dict())\n","    else:\n","        raise NotImplementedError\n","else:\n","    raise NotImplementedError\n","    logger.info(\"Training new model from scratch\")\n","    model = AutoModelForMaskedLM.from_config(config)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Su3oOLqpyrPj"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gc4oLHy3yrSK"},"outputs":[],"source":["model.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rs7P9Ixz1nxH"},"outputs":[],"source":["column_names = datasets[\"train\"].column_names\n","column_names"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6-zkZHGaN1_B"},"outputs":[],"source":["# Prepare features\n","\n","sent2_cname = None\n","if len(column_names) == 2:\n","    # Pair datasets\n","    sent0_cname = column_names[0]\n","    sent1_cname = column_names[1]\n","elif len(column_names) == 3:\n","    # Pair datasets with hard negatives\n","    sent0_cname = column_names[0]\n","    sent1_cname = column_names[1]\n","    sent2_cname = column_names[2]\n","elif len(column_names) == 1:\n","    # Unsupervised datasets\n","    sent0_cname = column_names[0]\n","    sent1_cname = column_names[0]\n","else:\n","    print('NotImplementedError')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RjrpYNK4Nk_P"},"outputs":[],"source":["sent0_cname = 'sent0'\n","sent1_cname = 'sent1'\n","sent2_cname = 'hard_neg'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YDwKQ8B9NlER"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hzEkNod7zTMP"},"outputs":[],"source":["def prepare_features(examples):\n","    # padding = longest (default)\n","    #   If no sentence in the batch exceed the max length, then use\n","    #   the max sentence length in the batch, otherwise use the \n","    #   max sentence length in the argument and truncate those that\n","    #   exceed the max length.\n","    # padding = max_length (when pad_to_max_length, for pressure test)\n","    #   All sentences are padded/truncated to data_args.max_seq_length.\n","    total = len(examples[sent0_cname])\n","\n","    sentence_pairs = []\n","    # Avoid \"None\" fields \n","    for idx in range(total):\n","        if examples[sent0_cname][idx] is None:\n","            examples[sent0_cname][idx] = \" \"\n","        if examples[sent1_cname][idx] is None:\n","            examples[sent1_cname][idx] = \" \"\n","            \n","    sentences = examples[sent0_cname] + examples[sent1_cname]\n","    #sentences = sentence_pairs\n","\n","    # If hard negative exists\n","    if sent2_cname is not None:\n","        for idx in range(total):\n","            if examples[sent2_cname][idx] is None:\n","                examples[sent2_cname][idx] = \" \"\n","        sentences += examples[sent2_cname]\n","\n","    sent_features = tokenizer(\n","        sentences,\n","        max_length=data_args.max_seq_length,\n","        truncation=True,\n","        padding=\"max_length\" if data_args.pad_to_max_length else False,\n","    )\n","    \n","    features = {}\n","    if sent2_cname is not None:\n","        for key in sent_features:\n","            features[key] = [[sent_features[key][i], sent_features[key][i+total], sent_features[key][i+total*2]] for i in range(total)]\n","    else:\n","        for key in sent_features: \n","            features[key] = [[sent_features[key][i], sent_features[key][i+total]] for i in range(total)]\n","\n","    return features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ip18QgXa0aua"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-iDlYN_IzTRC"},"outputs":[],"source":["train_dataset = None\n","if \"train\" in datasets:\n","  train_dataset = datasets[\"train\"].map(\n","      prepare_features,\n","      batched=True,\n","      num_proc=data_args.preprocessing_num_workers,\n","      remove_columns=column_names,\n","      load_from_cache_file=False,\n","  )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8qEnVKA4LVHF"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I3RtvRVrAAQT"},"outputs":[],"source":["eval_dataset = None\n","if \"validation\" in datasets:\n","  eval_dataset = datasets[\"validation\"].map(\n","      prepare_features,\n","      batched=True,\n","      num_proc=data_args.preprocessing_num_workers,\n","      remove_columns=column_names,\n","      load_from_cache_file=not data_args.overwrite_cache,\n","  )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z4TkAj0Zwk3L"},"outputs":[],"source":["predict_dataset = None\n","if \"test\" in datasets:\n","  predict_dataset = datasets[\"test\"].map(\n","      prepare_features,\n","      batched=True,\n","      num_proc=data_args.preprocessing_num_workers,\n","      remove_columns=column_names,\n","      load_from_cache_file=not data_args.overwrite_cache,\n","  )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"swTVzubKfTZR"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XjaeGKREzTT-"},"outputs":[],"source":["@dataclass\n","class OurDataCollatorWithPadding:\n","\n","    tokenizer: PreTrainedTokenizerBase\n","    padding: Union[bool, str, PaddingStrategy] = True\n","    max_length: Optional[int] = None\n","    pad_to_multiple_of: Optional[int] = None\n","    mlm: bool = True\n","    mlm_probability: float = data_args.mlm_probability\n","\n","    def __call__(self, features: List[Dict[str, Union[List[int], List[List[int]], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n","        special_keys = ['input_ids', 'attention_mask', 'token_type_ids', 'mlm_input_ids', 'mlm_labels']\n","        \n","        bs = len(features) # number of rows\n","        if bs > 0:\n","            num_sent = len(features[0]['input_ids']) # sentence pairs\n","        else:\n","            return\n","\n","        flat_features = []\n","        for feature in features:\n","            for i in range(num_sent):\n","                flat_features.append({k: feature[k][i] if k in special_keys else feature[k] for k in feature}) # all sentences in a batch\n","\n","        \n","        batch = self.tokenizer.pad(\n","            flat_features,\n","            padding=self.padding,\n","            max_length=self.max_length,\n","            pad_to_multiple_of=self.pad_to_multiple_of,\n","            return_tensors=\"pt\",\n","        )\n","        if model_args.do_mlm:\n","            batch[\"mlm_input_ids\"], batch[\"mlm_labels\"] = self.mask_tokens(batch[\"input_ids\"])\n","\n","        batch = {k: batch[k].view(bs, num_sent, -1) if k in special_keys else batch[k].view(bs, num_sent, -1)[:, 0] for k in batch}\n","\n","        if \"label\" in batch:\n","            batch[\"labels\"] = batch[\"label\"]\n","            del batch[\"label\"]\n","        if \"label_ids\" in batch:\n","            batch[\"labels\"] = batch[\"label_ids\"]\n","            del batch[\"label_ids\"]\n","\n","        return batch\n","    \n","    def mask_tokens(\n","        self, inputs: torch.Tensor, special_tokens_mask: Optional[torch.Tensor] = None\n","    ) -> Tuple[torch.Tensor, torch.Tensor]:\n","        \"\"\"\n","        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n","        \"\"\"\n","        inputs = inputs.clone()\n","        labels = inputs.clone()\n","        \n","        # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n","        \n","        if special_tokens_mask is None:\n","            special_tokens_mask = [\n","                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n","            ]\n","            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n","        else:\n","            special_tokens_mask = special_tokens_mask.bool()\n","        \n","        #\n","        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n","        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n","        \n","        masked_indices = torch.bernoulli(probability_matrix).bool()\n","        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n","\n","        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n","        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n","        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n","\n","        # 10% of the time, we replace masked input tokens with random word\n","        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n","        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n","        inputs[indices_random] = random_words[indices_random]\n","        \n","        \n","        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n","        return inputs, labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AjRCZ9ZmzTWy"},"outputs":[],"source":["data_collator = default_data_collator if data_args.pad_to_max_length else OurDataCollatorWithPadding(tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mudW6UMU10HE"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TpHiLCk6BUT7"},"outputs":[],"source":["data_args.pad_to_max_length"]},{"cell_type":"markdown","metadata":{"id":"p1BFPoxnzxtw"},"source":["# Build Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QGE4YzHzCFwA"},"outputs":[],"source":["#data_collator = DataCollatorForTokenClassification(tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zv3jkExuzTZE"},"outputs":[],"source":["trainer = CLTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset if do_train else None,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XNFi4m69IGH2"},"outputs":[],"source":["trainer.model_args = model_args"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nj3aoJ36ru9P"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"b8eYhzY6z6DL"},"source":["# Model Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jrjozoaWru_l"},"outputs":[],"source":["if do_train:\n","  train_result = trainer.train(model_path=model_args.model_name_or_path)\n","\n","  trainer.save_model()\n","  \n","  output_train_file = os.path.join(training_args.output_dir, \"train_results.txt\")\n","  if trainer.is_world_process_zero():\n","    with open(output_train_file, \"w\") as writer:\n","        logger.info(\"***** Train results *****\")\n","        for key, value in sorted(train_result.metrics.items()):\n","            logger.info(f\"  {key} = {value}\")\n","            writer.write(f\"{key} = {value}\\n\")\n","\n","    # Need to save the state, since Trainer.save_model saves only the tokenizer with the model\n","    trainer.state.save_to_json(os.path.join(training_args.output_dir, \"trainer_state.json\"))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T6gt1OVOnmJ8"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"_IwEbS7Bz9v7"},"source":["# Model Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7lgFvF5krvCA"},"outputs":[],"source":["if do_eval:\n","  results = trainer.evaluate()\n","  print(results)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K9o_vlte5L9A"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mwNLVX4gyrUu"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jcJnSDWitEIJ"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"machine_shape":"hm","collapsed_sections":["kAWxk8JdlPem","-tmYl4YrBDxB","p1BFPoxnzxtw","b8eYhzY6z6DL","_IwEbS7Bz9v7"],"authorship_tag":"ABX9TyPP8aJT1jqBRdFL4uQ/UB2w"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}